{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64503577",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import preprocessing as prep\n",
    "import machine_learning as ml\n",
    "from pipeline import AnomalyDetectionPipeline, load_and_evaluate_model\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c86b50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = {\n",
    "    'TOTF': {\n",
    "        'path': 'data/TOTF.PA-book/2015-01-02-TOTF.PA-book.csv.gz',\n",
    "        'type': 'standard_csv'\n",
    "    },\n",
    "\n",
    "    'LOBSTER': {\n",
    "        'orderbook': 'data/LOBSTER/AMZN_2012-06-21_34200000_57600000_orderbook_10.csv',\n",
    "        'message': 'data/LOBSTER/AMZN_2012-06-21_34200000_57600000_message_10.csv',\n",
    "        'type': 'lobster'\n",
    "    }\n",
    "}\n",
    "\n",
    "ALL_RESULTS = []\n",
    "ROC_DATA = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f50b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name, data_config in DATASETS.items():\n",
    "    print(f\"Processing dataset: {dataset_name}\")\n",
    "    \n",
    "    pipeline = AnomalyDetectionPipeline()\n",
    "    if data_config['type'] == 'lobster':\n",
    "        df = prep.load_lobster_data(\n",
    "            orderbook_path=data_config['orderbook'], \n",
    "            message_path=data_config['message'], \n",
    "            levels=10\n",
    "        )\n",
    "        pipeline.raw_df = df\n",
    "    else:\n",
    "        pipeline.load_data(data_config['path'])\n",
    "    \n",
    "    pipeline.engineer_features(feature_sets=['base', 'tao', 'hawkes', 'poutre', 'ofi'])\n",
    "\n",
    "    # Extract test data\n",
    "    test_start_idx = int(len(pipeline.processed_df) * 0.85)\n",
    "    test_df = pipeline.processed_df.iloc[test_start_idx:].reset_index(drop=True)\n",
    "    print(f\"Test data shape: {test_df.shape}\")\n",
    "\n",
    "    # Load saved models\n",
    "    saved_files = [f for f in os.listdir('models') if f.endswith('_config.json') an dataset_name in f]\n",
    "\n",
    "    for config_file in saved_files:\n",
    "        print(f\"Evaluating model from config: {config_file}\")\n",
    "        try:\n",
    "            results, y_true, y_scores, config = load_and_evaluate_model(\n",
    "                f\"models/{config_file}\",\n",
    "                test_df,\n",
    "                pipeline.feature_names\n",
    "            )\n",
    "\n",
    "            record = {\n",
    "                'dataset': dataset_name,\n",
    "                'model': config['model_type'],\n",
    "                'scaler': config['scaler'],\n",
    "                'auroc': results['AUROC'],\n",
    "                'auprc': results['AUPRC'],\n",
    "                'f4_score': results['F4_Score']\n",
    "            }\n",
    "\n",
    "            ALL_RESULTS.append(record)\n",
    "\n",
    "            key = f\"{config['model_type']} + {config['scaler_type']}\"\n",
    "            ROC_DATA[key] = (y_true, y_scores)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating model {config_file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc447635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results\n",
    "results_df = pd.DataFrame(ALL_RESULTS)\n",
    "results_df.sort_values(by='f4_score', ascending=False, inplace=True)\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88fda9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "sns.barplot(data=results_df, x='model', y='f4_score', hue='scaler', ax=ax[0], palette='viridis')\n",
    "ax[0].set_title(f'Model Performance (F4 Score) on {dataset_name}')\n",
    "ax[0].set_ylim(0, 1.1)\n",
    "\n",
    "for label, (y_test, y_score) in ROC_DATA.items():\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    ax[1].plot(fpr, tpr, label=f'{label} (AUC = {roc_auc:.3f})')\n",
    "ax[1].plot([0,1], [0,1], 'k--', lw=1)\n",
    "ax[1].set_xlabel('False Positive Rate')\n",
    "ax[1].set_ylabel('True Positive Rate')\n",
    "ax[1].set_title('ROC Curve Comparison')\n",
    "ax[1].legend(loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
