{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4b7d803",
   "metadata": {},
   "source": [
    "- Datasets: TOTF, LOBSTER\n",
    "- Scalers: MinMax, Box-Cox\n",
    "- Models: Transformer+OCSVM, PRAE, PNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c709f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import joblib\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import preprocessing as prep\n",
    "import machine_learning as ml\n",
    "from pipeline import AnomalyDetectionPipeline\n",
    "\n",
    "os.makedirs('models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99bca880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATASETS = {\n",
    "    'TOTF': {\n",
    "        'path': 'data/TOTF.PA-book/2015-01-02-TOTF.PA-book.csv.gz',\n",
    "        'type': 'standard_csv'\n",
    "    },\n",
    "\n",
    "    'LOBSTER': {\n",
    "        'orderbook': 'data/LOBSTER/AMZN_2012-06-21_34200000_57600000_orderbook_10.csv',\n",
    "        'message': 'data/LOBSTER/AMZN_2012-06-21_34200000_57600000_message_10.csv',\n",
    "        'type': 'lobster'\n",
    "    }\n",
    "}\n",
    "SCALERS = ['minmax', 'box-cox']\n",
    "MODELS = ['transformer_ocsvm', 'prae', 'pnn']\n",
    "\n",
    "# Hyperparameters\n",
    "SEQ_LENGTH = 25\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 50\n",
    "HIDDEN_DIM = 64\n",
    "LR = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0b32ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING DATASET: TOTF\n",
      "Pipeline initialized on device: cuda\n",
      "Loading data from data/TOTF.PA-book/2015-01-02-TOTF.PA-book.csv.gz...\n",
      "Successfully loaded 640429 rows.\n",
      "Loaded 640429 rows.\n",
      "Engineering features: ['base', 'tao', 'hawkes', 'poutre', 'ofi']...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(pipeline\u001b[38;5;241m.\u001b[39mraw_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m rows.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Engineer Features\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengineer_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_sets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbase\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtao\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhawkes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpoutre\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mofi\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m master_features_df \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mprocessed_df\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m scaler_type \u001b[38;5;129;01min\u001b[39;00m SCALERS:\n",
      "File \u001b[1;32mc:\\Users\\adoni\\Desktop\\Projet MDS\\MDS-Market_Manipulation\\pipeline.py:87\u001b[0m, in \u001b[0;36mAnomalyDetectionPipeline.engineer_features\u001b[1;34m(self, feature_sets)\u001b[0m\n\u001b[0;32m     84\u001b[0m     features \u001b[38;5;241m=\u001b[39m prep\u001b[38;5;241m.\u001b[39mcompute_order_flow_imbalance(df, data\u001b[38;5;241m=\u001b[39mfeatures)\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# Cleanup\u001b[39;00m\n\u001b[1;32m---> 87\u001b[0m \u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minf\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m features \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# Clip extreme outliers\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\adoni\\Desktop\\Projet MDS\\MDS-Market_Manipulation\\.venv\\lib\\site-packages\\pandas\\core\\generic.py:8125\u001b[0m, in \u001b[0;36mNDFrame.replace\u001b[1;34m(self, to_replace, value, inplace, limit, regex, method)\u001b[0m\n\u001b[0;32m   8120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(to_replace) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(value):\n\u001b[0;32m   8121\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   8122\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReplacement lists must match in length. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   8123\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(to_replace)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(value)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   8124\u001b[0m         )\n\u001b[1;32m-> 8125\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace_list\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   8126\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mto_replace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdest_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8128\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mregex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8130\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   8132\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m to_replace \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   8133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\n\u001b[0;32m   8134\u001b[0m         is_re_compilable(regex)\n\u001b[0;32m   8135\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m is_list_like(regex)\n\u001b[0;32m   8136\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m is_dict_like(regex)\n\u001b[0;32m   8137\u001b[0m     ):\n",
      "File \u001b[1;32mc:\\Users\\adoni\\Desktop\\Projet MDS\\MDS-Market_Manipulation\\.venv\\lib\\site-packages\\pandas\\core\\internals\\base.py:287\u001b[0m, in \u001b[0;36mDataManager.replace_list\u001b[1;34m(self, src_list, dest_list, inplace, regex)\u001b[0m\n\u001b[0;32m    276\u001b[0m inplace \u001b[38;5;241m=\u001b[39m validate_bool_kwarg(inplace, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minplace\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    278\u001b[0m bm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_with_block(\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplace_list\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    280\u001b[0m     src_list\u001b[38;5;241m=\u001b[39msrc_list,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    285\u001b[0m     already_warned\u001b[38;5;241m=\u001b[39m_AlreadyWarned(),\n\u001b[0;32m    286\u001b[0m )\n\u001b[1;32m--> 287\u001b[0m \u001b[43mbm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_consolidate_inplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m bm\n",
      "File \u001b[1;32mc:\\Users\\adoni\\Desktop\\Projet MDS\\MDS-Market_Manipulation\\.venv\\lib\\site-packages\\pandas\\core\\internals\\managers.py:1807\u001b[0m, in \u001b[0;36mBlockManager._consolidate_inplace\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1801\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_consolidate_inplace\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1802\u001b[0m     \u001b[38;5;66;03m# In general, _consolidate_inplace should only be called via\u001b[39;00m\n\u001b[0;32m   1803\u001b[0m     \u001b[38;5;66;03m#  DataFrame._consolidate_inplace, otherwise we will fail to invalidate\u001b[39;00m\n\u001b[0;32m   1804\u001b[0m     \u001b[38;5;66;03m#  the DataFrame's _item_cache. The exception is for newly-created\u001b[39;00m\n\u001b[0;32m   1805\u001b[0m     \u001b[38;5;66;03m#  BlockManager objects not yet attached to a DataFrame.\u001b[39;00m\n\u001b[0;32m   1806\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_consolidated():\n\u001b[1;32m-> 1807\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks \u001b[38;5;241m=\u001b[39m \u001b[43m_consolidate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1808\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1809\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_known_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\adoni\\Desktop\\Projet MDS\\MDS-Market_Manipulation\\.venv\\lib\\site-packages\\pandas\\core\\internals\\managers.py:2288\u001b[0m, in \u001b[0;36m_consolidate\u001b[1;34m(blocks)\u001b[0m\n\u001b[0;32m   2286\u001b[0m new_blocks: \u001b[38;5;28mlist\u001b[39m[Block] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   2287\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (_can_consolidate, dtype), group_blocks \u001b[38;5;129;01min\u001b[39;00m grouper:\n\u001b[1;32m-> 2288\u001b[0m     merged_blocks, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_merge_blocks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2289\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgroup_blocks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcan_consolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_can_consolidate\u001b[49m\n\u001b[0;32m   2290\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2291\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(merged_blocks, new_blocks)\n\u001b[0;32m   2292\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(new_blocks)\n",
      "File \u001b[1;32mc:\\Users\\adoni\\Desktop\\Projet MDS\\MDS-Market_Manipulation\\.venv\\lib\\site-packages\\pandas\\core\\internals\\managers.py:2323\u001b[0m, in \u001b[0;36m_merge_blocks\u001b[1;34m(blocks, dtype, can_consolidate)\u001b[0m\n\u001b[0;32m   2320\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m new_values[argsort]\n\u001b[0;32m   2321\u001b[0m     new_mgr_locs \u001b[38;5;241m=\u001b[39m new_mgr_locs[argsort]\n\u001b[1;32m-> 2323\u001b[0m     bp \u001b[38;5;241m=\u001b[39m \u001b[43mBlockPlacement\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_mgr_locs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [new_block_2d(new_values, placement\u001b[38;5;241m=\u001b[39mbp)], \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2326\u001b[0m \u001b[38;5;66;03m# can't consolidate --> no merge\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "for dataset_name, data_config in DATASETS.items():\n",
    "    print(f\"PROCESSING DATASET: {dataset_name}\")\n",
    "\n",
    "    pipeline = AnomalyDetectionPipeline(seq_length=SEQ_LENGTH, batch_size=BATCH_SIZE)\n",
    "\n",
    "    try:\n",
    "        # Load Data\n",
    "        if data_config['type'] == 'lobster':\n",
    "            df = prep.load_lobster_data(\n",
    "                orderbook_path=data_config['orderbook'], \n",
    "                message_path=data_config['message'], \n",
    "                levels=10\n",
    "            )\n",
    "            pipeline.raw_df = df\n",
    "        \n",
    "        else:\n",
    "            pipeline.load_data(data_config['path'])\n",
    "\n",
    "        print(f\"Loaded {len(pipeline.raw_df)} rows.\")\n",
    "\n",
    "        # Engineer Features\n",
    "        pipeline.engineer_features(feature_sets=['base', 'tao', 'hawkes', 'poutre', 'ofi'])\n",
    "        master_features_df = pipeline.processed_df.copy()\n",
    "\n",
    "        for scaler_type in SCALERS:\n",
    "            print(f\"Applying Scaler: {scaler_type}\")\n",
    "\n",
    "            for model_type in MODELS:\n",
    "                print(f\"Training Model: {model_type}\")\n",
    "\n",
    "                pipeline.processed_df = master_features_df.copy()\n",
    "                pipeline.feature_names = pipeline.processed_df.columns.tolist()\n",
    "\n",
    "                # Scale Features\n",
    "                pipeline.scale_and_sequence(method=scaler_type)\n",
    "\n",
    "                # Train Model\n",
    "                pipeline.train_model(\n",
    "                    model_type=model_type, \n",
    "                    epochs=EPOCHS, \n",
    "                    lr=LR,\n",
    "                    hidden_dim=HIDDEN_DIM\n",
    "                )\n",
    "\n",
    "                # Save Artifacts\n",
    "                base_filename = f\"models/{dataset_name}_{scaler_type}_{model_type}\"\n",
    "                \n",
    "                # Save Model Architecture\n",
    "                config = {\n",
    "                    'dataset': dataset_name,\n",
    "                    'model_type': model_type,\n",
    "                    'scaler_type': scaler_type,\n",
    "                    'input_dim': pipeline.X_train.shape[2],\n",
    "                    'seq_length': SEQ_LENGTH,\n",
    "                    'hidden_dim': HIDDEN_DIM,\n",
    "                    'batch_size': BATCH_SIZE,\n",
    "                    'epochs': EPOCHS,\n",
    "                    'learning_rate': LR,\n",
    "                    'feature_names': pipeline.feature_names,\n",
    "                    'train_samples': len(pipeline.X_train)\n",
    "                }\n",
    "                with open(f\"{base_filename}_config.json\", 'w') as f:\n",
    "                    json.dump(config, f, indent=4)\n",
    "\n",
    "                # Save Neural Network Weights\n",
    "                torch.save(pipeline.model.state_dict(), f\"{base_filename}_weights.pth\")\n",
    "\n",
    "                # Save Scaler\n",
    "                joblib.dump(pipeline.scaler, f\"{base_filename}_scaler.pkl\")\n",
    "\n",
    "                # Handle OC-SVM\n",
    "                if model_type == 'transformer_ocsvm':\n",
    "                    if pipeline.detector is not None:\n",
    "                        joblib.dump(pipeline.detector, f\"{base_filename}_ocsvm_detector.pkl\")\n",
    "                        joblib.dump(pipeline.ocsvm, f\"{base_filename}_latent_scaler.pkl\")\n",
    "\n",
    "                print(f\"Saved artifacts to {base_filename}\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"File not found for {dataset_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error processing {dataset_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef29f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "target_dataset = 'TOTF'\n",
    "target_scaler = 'box-cox'\n",
    "target_model = 'prae'\n",
    "base_path = f\"models/{target_dataset}_{target_scaler}_{target_model}\"\n",
    "\n",
    "if os.path.exists(f\"{base_path}_config.json\"):\n",
    "    # Load Config\n",
    "    with open(f\"{base_path}_config.json\", 'r') as f:\n",
    "        config = json.load(f)\n",
    "        \n",
    "    # Initialize Pipeline\n",
    "    test_pipeline = AnomalyDetectionPipeline(seq_length=config['seq_length'], batch_size=config['batch_size'])\n",
    "\n",
    "    # Load Scaler\n",
    "    test_pipeline.scaler = joblib.load(f\"{base_path}_scaler.pkl\")\n",
    "\n",
    "    # Initialize Model Architecture\n",
    "    input_dim = config['input_dim']\n",
    "    model_dim = config['hidden_dim']\n",
    "\n",
    "    if target_model == 'prae':\n",
    "        base_ae = ml.TransformerAutoencoder(num_features=input_dim, model_dim=model_dim, num_heads=2, representation_dim=128, seq_length=config['seq_length'])\n",
    "        test_pipeline.model = ml.ProbabilisticRobustAutoencoder(base_ae, num_train_samples=1)\n",
    "\n",
    "    # Load Model Weights\n",
    "    try:\n",
    "        test_pipeline.model.load_state_dict(torch.load(f\"{base_path}_weights.pth\"))\n",
    "        test_pipeline.model.eval()\n",
    "        print(\"Model and artifacts loaded successfully for verification.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model weights: {e}\")\n",
    "\n",
    "else:\n",
    "    print(f\"Configuration file not found at {base_path}_config.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
