{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4b7d803",
   "metadata": {},
   "source": [
    "- Datasets: TOTF, LOBSTER\n",
    "- Scalers: MinMax, Box-Cox\n",
    "- Models: Transformer+OCSVM, PRAE, PNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7c709f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import joblib\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import preprocessing as prep\n",
    "import machine_learning as ml\n",
    "from pipeline import AnomalyDetectionPipeline\n",
    "\n",
    "os.makedirs('models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99bca880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATASETS = {\n",
    "    'TOTF': {\n",
    "        'path': 'data/TOTF.PA-book/2015-01-02-TOTF.PA-book.csv.gz',\n",
    "        'type': 'standard_csv'\n",
    "    },\n",
    "\n",
    "    'LOBSTER': {\n",
    "        'orderbook': 'data/LOBSTER/AMZN_2012-06-21_34200000_57600000_orderbook_10.csv',\n",
    "        'message': 'data/LOBSTER/AMZN_2012-06-21_34200000_57600000_message_10.csv',\n",
    "        'type': 'lobster'\n",
    "    }\n",
    "}\n",
    "SCALERS = ['minmax', 'box-cox']\n",
    "MODELS = ['transformer_ocsvm', 'prae', 'pnn']\n",
    "\n",
    "# Hyperparameters\n",
    "SEQ_LENGTH = 25\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 50\n",
    "HIDDEN_DIM = 64\n",
    "LR = 1e-3\n",
    "PATIENCE = 5\n",
    "TRAIN_RATIO = 0.7\n",
    "VAL_RATIO = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d0b32ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING DATASET: TOTF\n",
      "Pipeline initialized on device: cuda\n",
      "Loading data from data/TOTF.PA-book/2015-01-02-TOTF.PA-book.csv.gz...\n",
      "Successfully loaded 640429 rows.\n",
      "Loaded 640429 rows.\n",
      "Engineering features: ['base', 'tao', 'hawkes', 'poutre', 'ofi']...\n",
      "Feature Engineering complete. Total features: 130\n",
      "Applying Scaler: minmax\n",
      "Training Model: transformer_ocsvm\n",
      "Preprocessing with method: minmax...\n",
      "Dropping 2 constant/zero-variance features: ['ask_sweep_cost', 'ask-volume-10']\n",
      "Data split: Train (448282, 25, 128), Val (96061, 25, 128), Test (96061, 25, 128)\n",
      "Initializing Transformer Autoencoder...\n",
      "Training Autoencoder (Max Epochs=50)...\n",
      "Epoch 1/50 - Train Loss: 0.005643 | Val Loss: 0.002906\n",
      "Unexpected error processing TOTF: 'EarlyStopping' object has no attribute 'save_checkpoint'\n",
      "PROCESSING DATASET: LOBSTER\n",
      "Pipeline initialized on device: cuda\n",
      "Loading LOBSTER data from data/LOBSTER/AMZN_2012-06-21_34200000_57600000_orderbook_10.csv...\n",
      "Successfully loaded 269748 LOBSTER rows.\n",
      "Loaded 269748 rows.\n",
      "Engineering features: ['base', 'tao', 'hawkes', 'poutre', 'ofi']...\n",
      "Feature Engineering complete. Total features: 129\n",
      "Applying Scaler: minmax\n",
      "Training Model: transformer_ocsvm\n",
      "Preprocessing with method: minmax...\n",
      "Dropping 12 constant/zero-variance features: ['Hawkes_L_ask_beta10_Eta10.0', 'Hawkes_L_bid_beta100_Eta10.0', 'Hawkes_L_bid_beta10_Eta10.0', 'Hawkes_L_bid_beta100_Eta1.0', 'Hawkes_L_ask_beta1000_Eta1.0', 'Hawkes_L_ask_beta1000_Eta10.0', 'Hawkes_L_bid_beta10_Eta1.0', 'Hawkes_L_ask_beta100_Eta1.0', 'Hawkes_L_ask_beta100_Eta10.0', 'Hawkes_L_ask_beta10_Eta1.0', 'Hawkes_L_bid_beta1000_Eta1.0', 'Hawkes_L_bid_beta1000_Eta10.0']\n",
      "Data split: Train (188806, 25, 117), Val (40458, 25, 117), Test (40459, 25, 117)\n",
      "Initializing Transformer Autoencoder...\n",
      "Training Autoencoder (Max Epochs=50)...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m pipeline\u001b[38;5;241m.\u001b[39mscale_and_sequence(method\u001b[38;5;241m=\u001b[39mscaler_type, train_ratio\u001b[38;5;241m=\u001b[39mTRAIN_RATIO, val_ratio\u001b[38;5;241m=\u001b[39mVAL_RATIO)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Train Model\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mHIDDEN_DIM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPATIENCE\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Save Artifacts\u001b[39;00m\n\u001b[0;32m     48\u001b[0m base_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscaler_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\adoni\\Desktop\\Projet MDS\\MDS-Market_Manipulation\\pipeline.py:238\u001b[0m, in \u001b[0;36mAnomalyDetectionPipeline.train_model\u001b[1;34m(self, model_type, epochs, lr, nu, hidden_dim, lambda_reg, patience)\u001b[0m\n\u001b[0;32m    236\u001b[0m batch_data \u001b[38;5;241m=\u001b[39m batch_data\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    237\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 238\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    239\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, batch_data)\n\u001b[0;32m    240\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\adoni\\Desktop\\Projet MDS\\MDS-Market_Manipulation\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\adoni\\Desktop\\Projet MDS\\MDS-Market_Manipulation\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\adoni\\Desktop\\Projet MDS\\MDS-Market_Manipulation\\machine_learning.py:95\u001b[0m, in \u001b[0;36mTransformerAutoencoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     94\u001b[0m     representation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x)\n\u001b[1;32m---> 95\u001b[0m     reconstructed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepresentation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m reconstructed\n",
      "File \u001b[1;32mc:\\Users\\adoni\\Desktop\\Projet MDS\\MDS-Market_Manipulation\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\adoni\\Desktop\\Projet MDS\\MDS-Market_Manipulation\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\adoni\\Desktop\\Projet MDS\\MDS-Market_Manipulation\\machine_learning.py:77\u001b[0m, in \u001b[0;36mBottleneckTransformerDecoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 77\u001b[0m     expanded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     expanded \u001b[38;5;241m=\u001b[39m expanded\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequence_length, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_dim)\n\u001b[0;32m     80\u001b[0m     decoded_seq \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_decoder(expanded)\n",
      "File \u001b[1;32mc:\\Users\\adoni\\Desktop\\Projet MDS\\MDS-Market_Manipulation\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\adoni\\Desktop\\Projet MDS\\MDS-Market_Manipulation\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\adoni\\Desktop\\Projet MDS\\MDS-Market_Manipulation\\.venv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "for dataset_name, data_config in DATASETS.items():\n",
    "    print(f\"PROCESSING DATASET: {dataset_name}\")\n",
    "\n",
    "    pipeline = AnomalyDetectionPipeline(seq_length=SEQ_LENGTH, batch_size=BATCH_SIZE)\n",
    "\n",
    "    try:\n",
    "        # Load Data\n",
    "        if data_config['type'] == 'lobster':\n",
    "            df = prep.load_lobster_data(\n",
    "                orderbook_path=data_config['orderbook'], \n",
    "                message_path=data_config['message'], \n",
    "                levels=10\n",
    "            )\n",
    "            pipeline.raw_df = df\n",
    "        \n",
    "        else:\n",
    "            pipeline.load_data(data_config['path'])\n",
    "\n",
    "        print(f\"Loaded {len(pipeline.raw_df)} rows.\")\n",
    "\n",
    "        # Engineer Features\n",
    "        pipeline.engineer_features(feature_sets=['base', 'tao', 'hawkes', 'poutre', 'ofi'])\n",
    "        master_features_df = pipeline.processed_df.copy()\n",
    "\n",
    "        for scaler_type in SCALERS:\n",
    "            print(f\"Applying Scaler: {scaler_type}\")\n",
    "\n",
    "            for model_type in MODELS:\n",
    "                print(f\"Training Model: {model_type}\")\n",
    "\n",
    "                pipeline.processed_df = master_features_df.copy()\n",
    "                pipeline.feature_names = pipeline.processed_df.columns.tolist()\n",
    "\n",
    "                # Scale Features\n",
    "                pipeline.scale_and_sequence(method=scaler_type, train_ratio=TRAIN_RATIO, val_ratio=VAL_RATIO)\n",
    "\n",
    "                # Train Model\n",
    "                pipeline.train_model(\n",
    "                    model_type=model_type, \n",
    "                    epochs=EPOCHS, \n",
    "                    lr=LR,\n",
    "                    hidden_dim=HIDDEN_DIM,\n",
    "                    patience=PATIENCE\n",
    "                )\n",
    "\n",
    "                # Save Artifacts\n",
    "                base_filename = f\"models/{dataset_name}_{scaler_type}_{model_type}\"\n",
    "                \n",
    "                # Save Model Architecture\n",
    "                config = {\n",
    "                    'dataset': dataset_name,\n",
    "                    'model_type': model_type,\n",
    "                    'scaler_type': scaler_type,\n",
    "                    'input_dim': pipeline.X_train.shape[2],\n",
    "                    'seq_length': SEQ_LENGTH,\n",
    "                    'hidden_dim': HIDDEN_DIM,\n",
    "                    'batch_size': BATCH_SIZE,\n",
    "                    'epochs': EPOCHS,\n",
    "                    'learning_rate': LR,\n",
    "                    'feature_names': pipeline.feature_names,\n",
    "                    'train_samples': len(pipeline.X_train)\n",
    "                }\n",
    "                with open(f\"{base_filename}_config.json\", 'w') as f:\n",
    "                    json.dump(config, f, indent=4)\n",
    "\n",
    "                # Save Neural Network Weights\n",
    "                torch.save(pipeline.model.state_dict(), f\"{base_filename}_weights.pth\")\n",
    "\n",
    "                # Save Scaler\n",
    "                joblib.dump(pipeline.scaler, f\"{base_filename}_scaler.pkl\")\n",
    "\n",
    "                # Handle OC-SVM\n",
    "                if model_type == 'transformer_ocsvm':\n",
    "                    if pipeline.detector is not None:\n",
    "                        joblib.dump(pipeline.detector, f\"{base_filename}_ocsvm_detector.pkl\")\n",
    "                        joblib.dump(pipeline.ocsvm, f\"{base_filename}_latent_scaler.pkl\")\n",
    "\n",
    "                print(f\"Saved artifacts to {base_filename}\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"File not found for {dataset_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error processing {dataset_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef29f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "target_dataset = 'TOTF'\n",
    "target_scaler = 'box-cox'\n",
    "target_model = 'prae'\n",
    "base_path = f\"models/{target_dataset}_{target_scaler}_{target_model}\"\n",
    "\n",
    "if os.path.exists(f\"{base_path}_config.json\"):\n",
    "    # Load Config\n",
    "    with open(f\"{base_path}_config.json\", 'r') as f:\n",
    "        config = json.load(f)\n",
    "        \n",
    "    # Initialize Pipeline\n",
    "    test_pipeline = AnomalyDetectionPipeline(seq_length=config['seq_length'], batch_size=config['batch_size'])\n",
    "\n",
    "    # Load Scaler\n",
    "    test_pipeline.scaler = joblib.load(f\"{base_path}_scaler.pkl\")\n",
    "\n",
    "    # Initialize Model Architecture\n",
    "    input_dim = config['input_dim']\n",
    "    model_dim = config['hidden_dim']\n",
    "\n",
    "    if target_model == 'prae':\n",
    "        base_ae = ml.TransformerAutoencoder(num_features=input_dim, model_dim=model_dim, num_heads=2, representation_dim=128, seq_length=config['seq_length'])\n",
    "        test_pipeline.model = ml.ProbabilisticRobustAutoencoder(base_ae, num_train_samples=1)\n",
    "\n",
    "    # Load Model Weights\n",
    "    try:\n",
    "        test_pipeline.model.load_state_dict(torch.load(f\"{base_path}_weights.pth\"))\n",
    "        test_pipeline.model.eval()\n",
    "        print(\"Model and artifacts loaded successfully for verification.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model weights: {e}\")\n",
    "\n",
    "else:\n",
    "    print(f\"Configuration file not found at {base_path}_config.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
