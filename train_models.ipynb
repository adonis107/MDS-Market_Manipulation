{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4b7d803",
   "metadata": {},
   "source": [
    "- Datasets: TOTF, LOBSTER\n",
    "- Scalers: MinMax, Box-Cox\n",
    "- Models: Transformer+OCSVM, PRAE, PNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c709f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import joblib\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import preprocessing as prep\n",
    "import machine_learning as ml\n",
    "from pipeline import AnomalyDetectionPipeline\n",
    "\n",
    "os.makedirs('models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bca880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATASETS = {\n",
    "    'TOTF': {\n",
    "        'path': 'data/TOTF.PA-book/2015-01-02-TOTF.PA-book.csv.gz',\n",
    "        'type': 'standard_csv'\n",
    "    },\n",
    "\n",
    "    'LOBSTER': {\n",
    "        'orderbook': 'data/LOBSTER/AMZN_2012-06-21_34200000_57600000_orderbook_10.csv',\n",
    "        'message': 'data/LOBSTER/AMZN_2012-06-21_34200000_57600000_message_10.csv',\n",
    "        'type': 'lobster'\n",
    "    }\n",
    "}\n",
    "SCALERS = ['minmax', 'box-cox']\n",
    "MODELS = ['transformer_ocsvm', 'prae', 'pnn']\n",
    "\n",
    "# Hyperparameters\n",
    "SEQ_LENGTH = 25\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 50\n",
    "HIDDEN_DIM = 64\n",
    "LR = 1e-3\n",
    "PATIENCE = 5\n",
    "TRAIN_RATIO = 0.7\n",
    "VAL_RATIO = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0b32ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "for dataset_name, data_config in DATASETS.items():\n",
    "    print(f\"PROCESSING DATASET: {dataset_name}\")\n",
    "\n",
    "    pipeline = AnomalyDetectionPipeline(seq_length=SEQ_LENGTH, batch_size=BATCH_SIZE)\n",
    "\n",
    "    try:\n",
    "        # Load Data\n",
    "        if data_config['type'] == 'lobster':\n",
    "            df = prep.load_lobster_data(\n",
    "                orderbook_path=data_config['orderbook'], \n",
    "                message_path=data_config['message'], \n",
    "                levels=10\n",
    "            )\n",
    "            pipeline.raw_df = df\n",
    "        \n",
    "        else:\n",
    "            pipeline.load_data(data_config['path'])\n",
    "\n",
    "        print(f\"Loaded {len(pipeline.raw_df)} rows.\")\n",
    "\n",
    "        # Engineer Features\n",
    "        pipeline.engineer_features(feature_sets=['base', 'tao', 'hawkes', 'poutre', 'ofi'])\n",
    "        master_features_df = pipeline.processed_df.copy()\n",
    "\n",
    "        for scaler_type in SCALERS:\n",
    "            print(f\"Applying Scaler: {scaler_type}\")\n",
    "\n",
    "            for model_type in MODELS:\n",
    "                print(f\"Training Model: {model_type}\")\n",
    "\n",
    "                pipeline.processed_df = master_features_df.copy()\n",
    "                pipeline.feature_names = pipeline.processed_df.columns.tolist()\n",
    "\n",
    "                # Scale Features\n",
    "                pipeline.scale_and_sequence(method=scaler_type, train_ratio=TRAIN_RATIO, val_ratio=VAL_RATIO)\n",
    "\n",
    "                # Train Model\n",
    "                pipeline.train_model(\n",
    "                    model_type=model_type, \n",
    "                    epochs=EPOCHS, \n",
    "                    lr=LR,\n",
    "                    hidden_dim=HIDDEN_DIM,\n",
    "                    patience=PATIENCE\n",
    "                )\n",
    "\n",
    "                # Save Artifacts\n",
    "                base_filename = f\"models/{dataset_name}_{scaler_type}_{model_type}\"\n",
    "                \n",
    "                # Save Model Architecture\n",
    "                config = {\n",
    "                    'dataset': dataset_name,\n",
    "                    'model_type': model_type,\n",
    "                    'scaler_type': scaler_type,\n",
    "                    'input_dim': pipeline.X_train.shape[2],\n",
    "                    'seq_length': SEQ_LENGTH,\n",
    "                    'hidden_dim': HIDDEN_DIM,\n",
    "                    'batch_size': BATCH_SIZE,\n",
    "                    'epochs': EPOCHS,\n",
    "                    'learning_rate': LR,\n",
    "                    'feature_names': pipeline.feature_names,\n",
    "                    'train_samples': len(pipeline.X_train)\n",
    "                }\n",
    "                with open(f\"{base_filename}_config.json\", 'w') as f:\n",
    "                    json.dump(config, f, indent=4)\n",
    "\n",
    "                # Save Neural Network Weights\n",
    "                torch.save(pipeline.model.state_dict(), f\"{base_filename}_weights.pth\")\n",
    "\n",
    "                # Save Scaler\n",
    "                joblib.dump(pipeline.scaler, f\"{base_filename}_scaler.pkl\")\n",
    "\n",
    "                # Handle OC-SVM\n",
    "                if model_type == 'transformer_ocsvm':\n",
    "                    if pipeline.detector is not None:\n",
    "                        joblib.dump(pipeline.detector, f\"{base_filename}_ocsvm_detector.pkl\")\n",
    "                        joblib.dump(pipeline.ocsvm, f\"{base_filename}_latent_scaler.pkl\")\n",
    "\n",
    "                print(f\"Saved artifacts to {base_filename}\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"File not found for {dataset_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error processing {dataset_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef29f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "target_dataset = 'TOTF'\n",
    "target_scaler = 'box-cox'\n",
    "target_model = 'prae'\n",
    "base_path = f\"models/{target_dataset}_{target_scaler}_{target_model}\"\n",
    "\n",
    "if os.path.exists(f\"{base_path}_config.json\"):\n",
    "    # Load Config\n",
    "    with open(f\"{base_path}_config.json\", 'r') as f:\n",
    "        config = json.load(f)\n",
    "        \n",
    "    # Initialize Pipeline\n",
    "    test_pipeline = AnomalyDetectionPipeline(seq_length=config['seq_length'], batch_size=config['batch_size'])\n",
    "\n",
    "    # Load Scaler\n",
    "    test_pipeline.scaler = joblib.load(f\"{base_path}_scaler.pkl\")\n",
    "\n",
    "    # Initialize Model Architecture\n",
    "    input_dim = config['input_dim']\n",
    "    model_dim = config['hidden_dim']\n",
    "\n",
    "    if target_model == 'prae':\n",
    "        base_ae = ml.TransformerAutoencoder(num_features=input_dim, model_dim=model_dim, num_heads=2, representation_dim=128, seq_length=config['seq_length'])\n",
    "        test_pipeline.model = ml.ProbabilisticRobustAutoencoder(base_ae, num_train_samples=1)\n",
    "\n",
    "    # Load Model Weights\n",
    "    try:\n",
    "        test_pipeline.model.load_state_dict(torch.load(f\"{base_path}_weights.pth\"))\n",
    "        test_pipeline.model.eval()\n",
    "        print(\"Model and artifacts loaded successfully for verification.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model weights: {e}\")\n",
    "\n",
    "else:\n",
    "    print(f\"Configuration file not found at {base_path}_config.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
